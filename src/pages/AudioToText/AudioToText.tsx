import React, { useState, useCallback } from 'react';
import { useTranslation } from 'react-i18next';
import { useDropzone } from 'react-dropzone';
import { useAuth } from '../../contexts/AuthContext';
import RecordingModal from '../../components/RecordingModal/RecordingModal';
import TranscriptionResult from '../../components/TranscriptionResult/TranscriptionResult';
import { transcribeAudio } from '../../services/audioService';
import { exportToWord } from '../../utils/exportUtils';
import { checkUsageLimit, recordUsage, truncateAudioForLimit, getAudioDuration } from '../../services/usageService';
import './AudioToText.css';

interface TranscriptionData {
  text: string;
  audioFile?: File;
}

const AudioToText: React.FC = () => {
  const { t } = useTranslation();
  const { user, isGuest, isAuthenticated, updateUserQuota, ensureGuestMode } = useAuth();
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcriptionResult, setTranscriptionResult] = useState<TranscriptionData | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [uploadedFile, setUploadedFile] = useState<File | null>(null);
  const [isRecordingModalOpen, setIsRecordingModalOpen] = useState(false);
  const [usageLimitWarning, setUsageLimitWarning] = useState<string | null>(null);

  // Restore transcription result on page load
  React.useEffect(() => {
    const storedTranscription = localStorage.getItem('transcriptionResult');
    if (storedTranscription) {
      setTranscriptionResult({
        text: storedTranscription,
        audioFile: undefined // File can't be serialized, so it's undefined on restore
      });
    }
  }, []);

  // Only ensure guest mode if explicitly in guest mode
  React.useEffect(() => {
    if (!user && localStorage.getItem('guestMode') === 'true') {
      console.log('ğŸ“„ AudioToTexté¡µé¢åŠ è½½ï¼Œå·²æœ‰è®¿å®¢æ¨¡å¼æ ‡è¯†ï¼Œæ›´æ–°è®¿å®¢çŠ¶æ€...');
      ensureGuestMode().catch(error => {
        console.error('âŒ è®¿å®¢æ¨¡å¼æ›´æ–°å¤±è´¥:', error);
      });
    }
  }, [user, ensureGuestMode]);

  const onDrop = useCallback(async (acceptedFiles: File[]) => {
    const file = acceptedFiles[0];
    if (file) {
      try {
        console.log('ğŸ“ ä¸Šä¼ æ–‡ä»¶:', file.name, 'å¤§å°:', (file.size / 1024 / 1024).toFixed(2), 'MB');
        
        // ç®€åŒ–å¤„ç†ï¼šç›´æ¥æ¥å—æ–‡ä»¶ï¼Œé…é¢æ£€æŸ¥äº¤ç»™è½¬å½•æ—¶å¤„ç†
        setUploadedFile(file);
        setError(null);
        setUsageLimitWarning(null);
        
        // é¢„å…ˆæ£€æŸ¥å¹¶ç»™å‡ºæç¤ºï¼ˆä½†ä¸é˜»æ­¢ä¸Šä¼ ï¼‰
        try {
          const fileDuration = await getAudioDuration(file);
          // ç»Ÿä¸€çš„é…é¢è®¡ç®—é€»è¾‘
          const isGuestUser = isGuest || !user || user?.userType === 'guest';
          const totalQuota = isGuestUser ? 5 : (user?.quotaMinutes || 10);
          const usedQuota = isGuestUser ? Number(localStorage.getItem('guestUsedMinutes') || '0') : (user?.usedMinutes || 0);
          const remainingMinutes = Math.max(0, totalQuota - usedQuota);
          
          if (fileDuration > remainingMinutes && remainingMinutes > 0) {
            setUsageLimitWarning(
              t('audioToText.audioDurationWarning', {
                duration: fileDuration.toFixed(1),
                remaining: remainingMinutes.toFixed(1)
              })
            );
          } else if (remainingMinutes <= 0) {
            setUsageLimitWarning(
              isGuestUser ? 
              t('audioToText.guestQuotaExhausted') :
              t('audioToText.quotaExhaustedGeneral')
            );
          }
        } catch (error) {
          console.warn('âš ï¸ é¢„æ£€æŸ¥å¤±è´¥ï¼Œä½†æ–‡ä»¶å·²ä¸Šä¼ :', error);
        }
        
      } catch (error) {
        console.error('File processing error:', error);
        setError('æ–‡ä»¶å¤„ç†é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®');
      }
    }
  }, [isGuest, user, t]);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'audio/*': ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.mp4', '.mpeg', '.mpga', '.webm']
    },
    multiple: false,
    // No maxSize limit - handle large files with chunking
  });

  const handleTranscription = async (audioFile: File) => {
    setIsProcessing(true);
    setError(null);
    setUsageLimitWarning(null);
    
    try {
      // ç»Ÿä¸€çš„ç”¨æˆ·ç±»å‹å’Œé…é¢æ£€æŸ¥
      const isGuestUser = isGuest || !user || user?.userType === 'guest';
      const userType = isGuestUser ? 'guest' : (user?.userType || 'trial');
      
      // è·å–å½“å‰ä½¿ç”¨é‡
      const currentUsage = isGuestUser ? Number(localStorage.getItem('guestUsedMinutes') || '0') : (user?.usedMinutes || 0);
      
      // è·å–éŸ³é¢‘æ—¶é•¿
      const originalDuration = await getAudioDuration(audioFile);
      
      // è®¡ç®—å‰©ä½™é…é¢
      const totalQuota = isGuestUser ? 5 : (user?.quotaMinutes || 10);
      const remainingMinutes = Math.max(0, totalQuota - currentUsage);
      
      console.log(`ğŸµ éŸ³é¢‘æ—¶é•¿: ${originalDuration.toFixed(2)}åˆ†é’Ÿ, å‰©ä½™é…é¢: ${remainingMinutes.toFixed(2)}åˆ†é’Ÿ`);
      console.log(`ğŸ‘¤ ç”¨æˆ·ç±»å‹: ${userType}, æ¸¸å®¢ç”¨æˆ·: ${isGuestUser}`);
      
      let actualAudioFile = audioFile;
      let actualDuration = originalDuration;
      let wasTruncated = false;
      
      // æ™ºèƒ½å¤„ç†ï¼šå¦‚æœæ²¡æœ‰å‰©ä½™é…é¢ï¼Œç›´æ¥æç¤ºç”¨æˆ·
      if (remainingMinutes <= 0) {
        setError(isGuestUser ? 
          t('audioToText.guestQuotaExhausted') : 
          t('audioToText.quotaExhaustedGeneral')
        );
        setIsProcessing(false);
        return;
      }
      
      // å¦‚æœéŸ³é¢‘æ—¶é•¿è¶…è¿‡å‰©ä½™é…é¢ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†ï¼ˆä½†ä»ç„¶å…è®¸è½¬å†™ï¼‰
      if (originalDuration > remainingMinutes) {
        console.log(`âš ï¸ éŸ³é¢‘è¶…å‡ºå‰©ä½™é…é¢ï¼Œå°†æˆªæ–­å¤„ç†: ${originalDuration.toFixed(2)} -> ${remainingMinutes.toFixed(2)}åˆ†é’Ÿ`);
        
        const truncateResult = await truncateAudioForLimit(audioFile, remainingMinutes);
        actualAudioFile = truncateResult.file;
        actualDuration = Math.min(originalDuration, remainingMinutes);
        wasTruncated = true;
        
        setUsageLimitWarning(
          `âš ï¸ éŸ³é¢‘æ—¶é•¿ ${originalDuration.toFixed(1)} åˆ†é’Ÿè¶…å‡ºå‰©ä½™é…é¢ ${remainingMinutes.toFixed(1)} åˆ†é’Ÿï¼Œä»…è½¬æ¢å‰ ${actualDuration.toFixed(1)} åˆ†é’Ÿå†…å®¹`
        );
      }
      
      // æ‰§è¡Œè½¬å½•ï¼ˆæ— è®ºæ˜¯å¦æˆªæ–­éƒ½å…è®¸è¿›è¡Œï¼‰
      const transcriptionText = await transcribeAudio(actualAudioFile, userType, currentUsage);
      
      // æ›´æ–°ä½¿ç”¨é‡ï¼ˆç»Ÿä¸€å¤„ç†æ¸¸å®¢å’Œè®¤è¯ç”¨æˆ·ï¼‰
      const newUsedMinutes = currentUsage + actualDuration;
      await updateUserQuota(newUsedMinutes);
      
      // æ£€æŸ¥é…é¢çŠ¶æ€å¹¶ç»™å‡ºç›¸åº”æç¤º
      const quotaLimit = isGuestUser ? 5 : (user?.quotaMinutes || 10);
      if (newUsedMinutes >= quotaLimit) {
        setUsageLimitWarning(
          isGuestUser ? 
          t('audioToText.guestTrialComplete') :
          t('audioToText.trialComplete')
        );
      } else if (quotaLimit - newUsedMinutes <= 1) {
        setUsageLimitWarning(`â° æ³¨æ„ï¼šæ‚¨è¿˜å‰©ä½™ ${(quotaLimit - newUsedMinutes).toFixed(1)} åˆ†é’Ÿé…é¢`);
      }
      
      // è®°å½•ä½¿ç”¨æƒ…å†µ
      await recordUsage(actualAudioFile, transcriptionText);
      
      const result: TranscriptionData = {
        text: transcriptionText,
        audioFile: audioFile
      };
      
      setTranscriptionResult(result);
      localStorage.setItem('transcriptionResult', result.text);
      
      console.log(`âœ… è½¬å½•å®Œæˆï¼Œä½¿ç”¨æ—¶é•¿: ${actualDuration.toFixed(2)}åˆ†é’Ÿ${wasTruncated ? ' (å·²æˆªæ–­)' : ''}`);
      
    } catch (error) {
      console.error('âŒ è½¬å½•å¤±è´¥:', error);
      setError(error instanceof Error ? error.message : 'è½¬å½•è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯');
    } finally {
      setIsProcessing(false);
    }
  };

  const handleStartTranscription = () => {
    if (uploadedFile) {
      handleTranscription(uploadedFile);
    }
  };

  // éŸ³é¢‘æ ¼å¼è½¬æ¢å‡½æ•°
  const convertAudioToSupportedFormat = async (audioBlob: Blob): Promise<File> => {
    const originalMimeType = audioBlob.type || 'audio/webm';
    console.log('ğŸµ åŸå§‹éŸ³é¢‘æ ¼å¼:', originalMimeType);
    
    // OpenAI API æ”¯æŒçš„æ ¼å¼åˆ—è¡¨
    const supportedFormats = [
      'audio/flac', 'audio/m4a', 'audio/mp3', 'audio/mp4', 
      'audio/mpeg', 'audio/mpga', 'audio/oga', 'audio/ogg', 
      'audio/wav', 'audio/webm'
    ];
    
    // æ£€æŸ¥å½“å‰æ ¼å¼æ˜¯å¦è¢«æ”¯æŒ
    const isDirectlySupported = supportedFormats.some(format => 
      originalMimeType.startsWith(format)
    );
    
    if (isDirectlySupported) {
      // æ ¼å¼å·²æ”¯æŒï¼Œç›´æ¥ä½¿ç”¨
      const fileExtension = getFileExtension(originalMimeType);
      const cleanMimeType = getCleanMimeType(originalMimeType);
      console.log('âœ… æ ¼å¼å·²æ”¯æŒï¼Œç›´æ¥ä½¿ç”¨:', cleanMimeType);
      return new File([audioBlob], `recording.${fileExtension}`, { type: cleanMimeType });
    }
    
    // ä¸æ”¯æŒçš„æ ¼å¼ï¼Œè½¬æ¢ä¸ºWAVï¼ˆæœ€å…¼å®¹çš„æ ¼å¼ï¼‰
    console.log('ğŸ”„ æ ¼å¼ä¸æ”¯æŒï¼Œè½¬æ¢ä¸º WAV...');
    try {
      const wavFile = await convertToWav(audioBlob);
      console.log('âœ… æˆåŠŸè½¬æ¢ä¸º WAV æ ¼å¼ï¼Œå¤§å°:', wavFile.size, 'bytes');
      return wavFile;
    } catch (error) {
      console.error('âŒ æ ¼å¼è½¬æ¢å¤±è´¥:', error);
      // è½¬æ¢å¤±è´¥ï¼Œä»ç„¶å°è¯•ä½¿ç”¨åŸå§‹æ ¼å¼ï¼ˆå¯èƒ½APIèƒ½å¤„ç†ï¼‰
      const fileExtension = getFileExtension(originalMimeType);
      return new File([audioBlob], `recording.${fileExtension}`, { type: 'audio/webm' });
    }
  };
  
  // è·å–æ–‡ä»¶æ‰©å±•å
  const getFileExtension = (mimeType: string): string => {
    if (mimeType.includes('wav')) return 'wav';
    if (mimeType.includes('mp4')) return 'mp4';
    if (mimeType.includes('mpeg')) return 'mpeg';
    if (mimeType.includes('ogg')) return 'ogg';
    if (mimeType.includes('webm')) return 'webm';
    return 'webm'; // é»˜è®¤
  };
  
  // è·å–å¹²å‡€çš„MIMEç±»å‹ï¼ˆå»é™¤codecä¿¡æ¯ï¼‰
  const getCleanMimeType = (mimeType: string): string => {
    if (mimeType.includes('wav')) return 'audio/wav';
    if (mimeType.includes('mp4')) return 'audio/mp4';
    if (mimeType.includes('mpeg')) return 'audio/mpeg';
    if (mimeType.includes('ogg')) return 'audio/ogg';
    if (mimeType.includes('webm')) return 'audio/webm';
    return 'audio/webm'; // é»˜è®¤
  };
  
  // è½¬æ¢ä¸ºWAVæ ¼å¼
  const convertToWav = async (audioBlob: Blob): Promise<File> => {
    return new Promise((resolve, reject) => {
      const audio = new Audio();
      const url = URL.createObjectURL(audioBlob);
      
      audio.onloadeddata = async () => {
        try {
          const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
          const arrayBuffer = await audioBlob.arrayBuffer();
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
          
          // è½¬æ¢ä¸ºWAV
          const wavBlob = await audioBufferToWav(audioBuffer);
          const wavFile = new File([wavBlob], 'recording.wav', { type: 'audio/wav' });
          
          audioContext.close();
          URL.revokeObjectURL(url);
          resolve(wavFile);
        } catch (error) {
          URL.revokeObjectURL(url);
          reject(error);
        }
      };
      
      audio.onerror = () => {
        URL.revokeObjectURL(url);
        reject(new Error('æ— æ³•è§£ç éŸ³é¢‘æ•°æ®'));
      };
      
      audio.src = url;
    });
  };
  
  // å°†AudioBufferè½¬æ¢ä¸ºWAV Blob
  const audioBufferToWav = (audioBuffer: AudioBuffer): Promise<Blob> => {
    return new Promise((resolve) => {
      const numberOfChannels = audioBuffer.numberOfChannels;
      const sampleRate = audioBuffer.sampleRate;
      const format = 1; // PCM
      const bitDepth = 16;
      
      const bytesPerSample = bitDepth / 8;
      const blockAlign = numberOfChannels * bytesPerSample;
      const byteRate = sampleRate * blockAlign;
      const dataSize = audioBuffer.length * blockAlign;
      const bufferSize = 44 + dataSize;
      
      const arrayBuffer = new ArrayBuffer(bufferSize);
      const dataView = new DataView(arrayBuffer);
      
      // WAV æ–‡ä»¶å¤´
      let offset = 0;
      
      // RIFF chunk descriptor
      writeString(dataView, offset, 'RIFF'); offset += 4;
      dataView.setUint32(offset, 36 + dataSize, true); offset += 4;
      writeString(dataView, offset, 'WAVE'); offset += 4;
      
      // FMT sub-chunk
      writeString(dataView, offset, 'fmt '); offset += 4;
      dataView.setUint32(offset, 16, true); offset += 4;
      dataView.setUint16(offset, format, true); offset += 2;
      dataView.setUint16(offset, numberOfChannels, true); offset += 2;
      dataView.setUint32(offset, sampleRate, true); offset += 4;
      dataView.setUint32(offset, byteRate, true); offset += 4;
      dataView.setUint16(offset, blockAlign, true); offset += 2;
      dataView.setUint16(offset, bitDepth, true); offset += 2;
      
      // Data sub-chunk
      writeString(dataView, offset, 'data'); offset += 4;
      dataView.setUint32(offset, dataSize, true); offset += 4;
      
      // Write PCM samples
      for (let i = 0; i < audioBuffer.length; i++) {
        for (let channel = 0; channel < numberOfChannels; channel++) {
          const sample = Math.max(-1, Math.min(1, audioBuffer.getChannelData(channel)[i]));
          const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
          dataView.setInt16(offset, intSample, true);
          offset += 2;
        }
      }
      
      resolve(new Blob([arrayBuffer], { type: 'audio/wav' }));
    });
  };
  
  // å†™å…¥å­—ç¬¦ä¸²åˆ°DataView
  const writeString = (dataView: DataView, offset: number, string: string): void => {
    for (let i = 0; i < string.length; i++) {
      dataView.setUint8(offset + i, string.charCodeAt(i));
    }
  };

  const handleRecordingComplete = async (audioBlob: Blob) => {
    console.log('ğŸ™ï¸ handleRecordingComplete è¢«è°ƒç”¨ï¼ŒéŸ³é¢‘æ•°æ®å¤§å°:', audioBlob.size, 'bytes');
    
    if (audioBlob.size === 0) {
      console.error('âŒ éŸ³é¢‘æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè½¬å½•');
      setError('å½•éŸ³æ•°æ®ä¸ºç©ºï¼Œè¯·é‡æ–°å½•åˆ¶');
      return;
    }
    
    try {
      // è½¬æ¢ä¸ºAPIæ”¯æŒçš„æ ¼å¼
      const audioFile = await convertAudioToSupportedFormat(audioBlob);
      console.log('ğŸ“ æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶:', audioFile.name, 'å¤§å°:', audioFile.size, 'bytes', 'æ ¼å¼:', audioFile.type);
      
      console.log('ğŸš€ å‡†å¤‡å¼€å§‹è½¬å½•...');
      handleTranscription(audioFile);
    } catch (error) {
      console.error('âŒ éŸ³é¢‘æ ¼å¼å¤„ç†å¤±è´¥:', error);
      setError('éŸ³é¢‘æ ¼å¼å¤„ç†å¤±è´¥ï¼Œè¯·é‡æ–°å½•åˆ¶');
    }
  };

  // æ£€æŸ¥æµè§ˆå™¨å…¼å®¹æ€§çš„å‡½æ•°
  const checkBrowserCompatibility = () => {
    const errors = [];
    
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      errors.push('âŒ æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒåª’ä½“è®¾å¤‡è®¿é—® (getUserMedia)');
    }
    
    if (!window.MediaRecorder) {
      errors.push('âŒ æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒåª’ä½“å½•åˆ¶ (MediaRecorder)');
    }
    
    if (!window.AudioContext && !(window as any).webkitAudioContext) {
      errors.push('âŒ æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒéŸ³é¢‘å¤„ç† (AudioContext)');
    }
    
    return errors;
  };

  const handleOpenRecordingModal = () => {
    // åœ¨æ‰“å¼€å½•éŸ³å¼¹çª—å‰æ£€æŸ¥æµè§ˆå™¨å…¼å®¹æ€§
    const compatibilityErrors = checkBrowserCompatibility();
    if (compatibilityErrors.length > 0) {
      const errorMessage = 'ğŸš« å½•éŸ³åŠŸèƒ½ä¸å¯ç”¨:\n\n' + 
                          compatibilityErrors.join('\n') + 
                          '\n\nğŸ’¡ å»ºè®®ä½¿ç”¨ä»¥ä¸‹æµè§ˆå™¨:\n' +
                          'â€¢ Chrome 60+\n' +
                          'â€¢ Firefox 55+\n' +
                          'â€¢ Safari 14+\n' +
                          'â€¢ Edge 79+';
      alert(errorMessage);
      return;
    }
    
    setIsRecordingModalOpen(true);
  };

  const handleCloseRecordingModal = () => {
    setIsRecordingModalOpen(false);
  };

  const handleClearRecord = () => {
    setTranscriptionResult(null);
    setUploadedFile(null);
    setError(null);
    localStorage.removeItem('transcriptionResult');
  };

  const handleClearAll = () => {
    // Clear all content including uploaded file and transcription results
    setTranscriptionResult(null);
    setUploadedFile(null);
    setError(null);
    setIsProcessing(false);
    localStorage.removeItem('transcriptionResult');
  };

  const handleExportToWord = () => {
    if (transcriptionResult) {
      exportToWord(transcriptionResult.text, 'transcription');
    }
  };


  return (
    <div className="audio-to-text-page">
      <div className="container">
        <div className="page-header">
          <h1 className="page-title">{t('audioToText.title')}</h1>
          <p className="page-subtitle">
            {user ? (
              (() => {
                const isGuestUser = isGuest || user.userType === 'guest';
                const totalQuota = isGuestUser ? 5 : (user.quotaMinutes || 10);
                const usedQuota = isGuestUser ? Number(localStorage.getItem('guestUsedMinutes') || '0') : (user.usedMinutes || 0);
                const remainingTime = Math.max(0, totalQuota - usedQuota);
                return t('audioToText.remainingTime', { minutes: remainingTime.toFixed(1) });
              })()
            ) : t('audioToText.subtitle')}
          </p>
        </div>

        <div className="main-content">
          {/* Left side - Input section */}
          <div className="input-section">
            <div className="card">
              <h2>
                {t('audioToText.audioInputMethod')}
              </h2>
              
              {/* Upload Section */}
              <div className="audio-upload-area">
                <h3>
                  {t('audioToText.uploadAudio')}
                </h3>
                <div
                  {...getRootProps()}
                  className={`dropzone ${isDragActive ? 'active' : ''} ${uploadedFile ? 'has-file' : ''}`}
                >
                  <input {...getInputProps()} />
                  {uploadedFile ? (
                    <div className="uploaded-file">
                      <div className="file-icon" style={{ fontSize: '32px', color: 'var(--success-green)' }}>â™ª</div>
                      <div className="file-info">
                        <div className="file-name">{uploadedFile.name}</div>
                        <div className="file-size">
                          {(uploadedFile.size / (1024 * 1024)).toFixed(2)} MB
                        </div>
                        <div className="file-status" style={{ color: 'var(--success-green)', fontSize: 'var(--font-size-footnote)', marginTop: 'var(--spacing-xs)' }}>
                          {t('audioToText.fileUploaded')}
                        </div>
                      </div>
                    </div>
                  ) : (
                    <div className="dropzone-content">
                      <div className="upload-icon">ğŸ“</div>
                      <p>{isDragActive ? t('audioToText.dragDropActiveText') : t('audioToText.dragDropText')}</p>
                      <p className="file-info">{t('audioToText.supportedFormats')}</p>
                    </div>
                  )}
                </div>
              </div>

              {/* Divider */}
              <div className="input-divider">
                <div className="divider-line"></div>
                <span className="divider-text">{t('audioToText.dividerOr')}</span>
                <div className="divider-line"></div>
              </div>

              {/* Record Section */}
              <div className="audio-record-area">
                <h3>
                  {t('audioToText.liveRecording')}
                </h3>
                <div className="record-controls">
                  <button 
                    onClick={handleOpenRecordingModal}
                    className="button button-primary start-recording-button"
                  >
                    <span className="mic-icon">ğŸ™ï¸</span>
                    {t('audioToText.liveRecording')}
                  </button>
                </div>
              </div>
            </div>
          </div>

          {/* Right side - Output section */}
          <div className="output-section">
            <div className="output-container">
              {/* Output textarea - always visible */}
              <div className="output-area card">
                <textarea
                  value={transcriptionResult?.text || ''}
                  readOnly
                  className="output-textarea"
                  placeholder={transcriptionResult ? t('audioToText.transcriptionComplete') : t('audioToText.outputPlaceholder')}
                />
              </div>
              
              {/* Control buttons outside the textarea */}
              <div className="output-controls">
                <button
                  onClick={handleStartTranscription}
                  className={`button action-button button-primary ${isProcessing ? 'button-processing' : ''}`}
                  disabled={!uploadedFile || isProcessing}
                >
                  {isProcessing ? t('audioToText.processing') : t('audioToText.startTranscription')}
                </button>
                <button
                  onClick={() => {
                    transcriptionResult && navigator.clipboard.writeText(transcriptionResult.text);
                  }}
                  className="button action-button button-secondary"
                  disabled={!transcriptionResult || isGuest}
                >
                  {t('common.copy')}
                </button>
                <button
                  onClick={() => {
                    handleExportToWord();
                  }}
                  className="button action-button button-secondary"
                  disabled={!transcriptionResult}
                >
                  {t('common.export')}
                </button>
                <button
                  onClick={handleClearAll}
                  className="button action-button button-warning"
                  disabled={!uploadedFile && !transcriptionResult}
                >
                  {t('common.clear')}
                </button>
              </div>
            </div>
          </div>
        </div>

        {isProcessing && (
          <div className="processing-indicator card">
            <div className="loading-spinner"></div>
            <p>{t('audioToText.processing')}</p>
            {uploadedFile && uploadedFile.size > 25 * 1024 * 1024 && (
              <p style={{ fontSize: 'var(--font-size-footnote)', color: 'var(--text-secondary)', marginTop: 'var(--spacing-sm)' }}>
                å¤§æ–‡ä»¶æ­£åœ¨åˆ†æ®µå¤„ç†ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...
              </p>
            )}
          </div>
        )}

        {usageLimitWarning && (
          <div className="warning-message card" style={{ backgroundColor: 'var(--warning-orange)', color: 'white' }}>
            <p>{usageLimitWarning}</p>
          </div>
        )}

        {error && (
          <div className="error-message card" style={{ backgroundColor: 'var(--error-red)', color: 'white' }}>
            <p>{error}</p>
          </div>
        )}

        {/* Recording Modal */}
        <RecordingModal
          isOpen={isRecordingModalOpen}
          onClose={handleCloseRecordingModal}
          onRecordingComplete={handleRecordingComplete}
        />
      </div>
    </div>
  );
};

export default AudioToText;